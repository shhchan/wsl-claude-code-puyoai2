# チケット020: AI性能改善（連鎖構築強化）

## 概要

チケット019で実装した3つの高度AIモデル（連鎖探索型・強化学習型・人間行動学習型）の性能改善を実施する。現在のAIはランダムAIレベルの連鎖構築能力しかないため、実用的な連鎖構築が可能なレベルまで改良する。

## 問題分析

### 現在の問題点
1. **モデル全体**
   - ネクスト・ネクネクの情報がAI意思決定に活用されていない
   - `AIBase`クラスの`GameState`にnext_queueが未実装

2. **連鎖探索型AI (ChainSearchAI)**
   - 評価関数が単純すぎて連鎖構築に向かない
   - U字型の連鎖構築型（経験則）が考慮されていない
   - ハイパーパラメータがハードコード化されており調整できない

3. **強化学習型AI (RLPlayerAI)**
   - 事前学習データが活用できていない
   - 学習過程の可視化が不十分
   - 報酬関数が連鎖構築を適切に評価していない

4. **人間行動学習AI (HumanLearningAI)**
   - 学習データが不足または活用されていない
   - 人間の連鎖構築パターンを学習できていない

## 詳細タスク

### 1. AIBase・GameStateの拡張

#### 1.1 GameState強化
- [ ] next_queueフィールドの実装（ネクスト・ネクネク情報）
- [ ] フィールド分析情報の追加（連鎖ポテンシャル等）
- [ ] 過去の配置履歴情報の追加

#### 1.2 AIBase機能拡張
- [ ] next情報を活用するための統一インターフェース
- [ ] 配置可能性の高度な判定機能
- [ ] デバッグ情報の充実

### 2. ChainSearchAI改善

#### 2.1 評価関数の高度化
- [ ] U字型連鎖構築パターンの評価実装
- [ ] 連鎖発火可能性の詳細計算
- [ ] 多段連鎖構築ポテンシャル評価
- [ ] 色バランス考慮の評価関数

#### 2.2 パラメータ管理システム
- [ ] YAML設定ファイル対応（`config/chain_search_params.yaml`）
- [ ] ハイパーパラメータの外部化
- [ ] パラメータ自動調整機能
- [ ] A/Bテスト機能

#### 2.3 探索アルゴリズム改良
- [ ] より深い探索深度への対応（最大7手）
- [ ] 枝刈りアルゴリズムの最適化
- [ ] ネクスト情報を活用した先読み探索

### 3. RLPlayerAI改善

#### 3.1 学習システム強化
- [ ] 事前学習済みモデルの読み込み機能修正
- [ ] 学習データの保存・復元機能
- [ ] オンライン学習とオフライン学習の併用

#### 3.2 報酬関数の改良
- [ ] 連鎖構築段階での中間報酬
- [ ] 連鎖発火時の大幅報酬増加
- [ ] 長期戦略への報酬設計

#### 3.3 状態表現の改善
- [ ] ネクスト情報を含んだ状態エンコーディング
- [ ] フィールドの構造的特徴量抽出
- [ ] 連鎖ポテンシャルの状態表現

### 4. HumanLearningAI改善

#### 4.1 学習データ拡充
- [ ] 人間プレイデータの収集システム整備
- [ ] 高レベルプレイヤーのデータ重点収集
- [ ] データクリーニング・前処理の改善

#### 4.2 学習アルゴリズム改良
- [ ] 連鎖構築パターンの抽出・分類
- [ ] 状況別戦略の学習
- [ ] 模倣精度の向上

### 5. 共通インフラ改善

#### 5.1 性能評価システム
- [ ] 連鎖構築能力の定量評価指標
- [ ] AI間の定期的な性能比較テスト
- [ ] 長時間プレイでの安定性評価

#### 5.2 設定管理システム
- [ ] 各AIの設定ファイル化
- [ ] 設定のバージョン管理
- [ ] 設定変更の動的反映

### 6. テスト・検証

#### 6.1 性能テスト
- [ ] 各AIの連鎖構築能力測定
- [ ] RandomAIとの比較評価
- [ ] 実際のゲームプレイでの性能確認

#### 6.2 統合テスト
- [ ] game_controller.pyでの動作確認
- [ ] 長時間プレイでの安定性確認
- [ ] エラーハンドリングの確認

## 実装計画

### フェーズ1: 基盤強化 (3-4時間)
1. GameStateのnext_queue実装
2. AIBaseの機能拡張
3. 設定管理システムの構築

### フェーズ2: ChainSearchAI改善 (4-5時間)
1. 評価関数の高度化
2. YAML設定対応
3. U字型評価の実装

### フェーズ3: RLPlayerAI/HumanLearningAI改善 (3-4時間)
1. 学習システムの修正
2. 報酬関数の改良
3. データ管理の改善

### フェーズ4: 統合・テスト (2-3時間)
1. 各AIの統合テスト
2. 性能比較評価
3. ドキュメント更新

## ファイル構成

### 新規作成ファイル
- `config/ai_params/chain_search.yaml`: ChainSearchAI設定
- `config/ai_params/rl_player.yaml`: RLPlayerAI設定  
- `config/ai_params/human_learning.yaml`: HumanLearningAI設定
- `cpp/ai/ai_utils.h/.cpp`: AI共通ユーティリティ
- `cpp/ai/chain_evaluator.h/.cpp`: 連鎖評価専用クラス
- `python/ai/performance_monitor.py`: AI性能監視ツール

### 更新対象ファイル
- `cpp/ai/ai_base.h`: GameState拡張
- `cpp/ai/chain_search_ai.h`: 評価関数改良
- `cpp/ai/rl_player_ai.h`: 学習システム修正
- `cpp/ai/human_learning_ai.h`: データ管理改善
- `python/ui/game_controller.py`: 改善されたAI統合

### テストファイル
- `tests/test_improved_ai_020.py`: 改善後AI統合テスト
- `tests/performance_test_020.py`: AI性能比較テスト
- `docs/test-results-020.md`: 詳細テスト結果記録

## 技術仕様

### 連鎖構築評価指標
- **連鎖発火率**: 3連鎖以上を発火する確率
- **平均連鎖数**: 1ゲームあたりの平均連鎖数
- **最大連鎖数**: 達成可能な最大連鎖数
- **連鎖構築時間**: 連鎖を組むまでの平均ターン数

### パフォーマンス要件
- **思考時間**: 1手あたり500ms以内（既存維持）
- **連鎖構築率**: 70%以上のゲームで3連鎖以上達成
- **平均連鎖数**: 4連鎖以上
- **ランダムAI比**: スコアで3倍以上の優位性

## 依存関係

- チケット019（高度AIシステム実装）完了
- 既存のAIフレームワーク
- Python UI統合システム

## 優先度

高

## 推定工数

12-16時間（段階的実装）
- フェーズ1: 3-4時間
- フェーズ2: 4-5時間  
- フェーズ3: 3-4時間
- フェーズ4: 2-3時間

## 完了条件

- [ ] 各AIが3連鎖以上を安定して構築できる
- [ ] RandomAIと比較して明確な性能向上を示す
- [ ] ネクスト・ネクネク情報がAI判断に活用されている
- [ ] ChainSearchAIがYAML設定ファイルでパラメータ調整可能
- [ ] RLPlayerAI・HumanLearningAIが学習データを適切に活用
- [ ] game_controller.pyで改善されたAIが正常動作する
- [ ] 長時間プレイでの安定性が確認されている
- [ ] 性能評価レポートが作成されている

## 期待される成果

### 定量的成果
- 平均連鎖数: 2連鎖 → 4連鎖以上
- 連鎖発火率: 30% → 70%以上
- 対RandomAIスコア比: 1.2倍 → 3倍以上

### 定性的成果
- 実用的な連鎖構築戦略の実現
- AIの思考過程の可視化向上
- パラメータ調整による性能最適化
- より人間らしい戦略の実現

## 備考

- 性能改善は段階的に実施し、各フェーズで評価を行う
- パラメータ調整は実験的アプローチで最適値を探索
- 学習データの収集は継続的に実施
- 将来的な更なる改善への拡張性を考慮した設計

**実装開始予定**: 即座
**予定完了**: 2-3週間以内

---

## 実装ログ

*実装開始日: [実装時に記録]*
*実装完了日: [実装時に記録]*
*テスト結果: [テスト実施時に記録]*